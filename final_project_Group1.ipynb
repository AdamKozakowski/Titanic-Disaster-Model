{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1S8EghnyjxYg"
   },
   "source": [
    "# Titanic disaster model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inrotuction\n",
    "#### to do:\n",
    "- briefly describe model\n",
    "- talk about recent methods used for problems like this\\\n",
    "- talk about methods we want to use\n",
    "- what do we expect\n",
    "\n",
    "https://www.kaggle.com/competitions/titanic\n",
    "\n",
    "https://www.kaggle.com/code/alexisbcook/titanic-tutorial/notebook\n",
    "\n",
    "https://www.overleaf.com/read/fqxnygwqtnjs#fbe3c8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zxNBdgCNCZSn"
   },
   "source": [
    "### Seting up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "from scipy import optimize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.optimize import minimize, LinearConstraint\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the dataset\n",
    "The dataset used in the project was downloaded from github repository provided by Kaggle: https://raw.githubusercontent.com/dsindy/kaggle-titanic/master/data/. \n",
    "\n",
    "It came with data dictionary with definitions of values and keys of the dataset. From left variables were as fallows: survival - Survival result (0 = No, 1 = Yes), Pclass\t- ticket class\t(1 = 1st, 2 = 2nd, 3 = 3rd), Sex - sex,\n",
    "Age\t   -     age in years, \n",
    "Sibsp\t-    number of siblings / spouses aboard the Titanic, \t\n",
    "Parch\t-     number of parents / children aboard the Titanic\t\n",
    "Ticket\t-    ticket number, \n",
    "Fare\t-    passenger fare\t\n",
    "Cabin\t-    cabin number\t\n",
    "Embarked -\tport of Embarkation\tC = Cherbourg, Q = Queenstown, S = Southampton.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 666
    },
    "executionInfo": {
     "elapsed": 263,
     "status": "ok",
     "timestamp": 1700692440573,
     "user": {
      "displayName": "Kirill Korzuk",
      "userId": "04732590717018364342"
     },
     "user_tz": -180
    },
    "id": "C0ZWTfkFj8XM",
    "outputId": "5c778832-e116-4fb5-96f0-eef0058f510c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>887</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Montvila, Rev. Juozas</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>211536</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>888</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Graham, Miss. Margaret Edith</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112053</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>B42</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>889</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnston, Miss. Catherine Helen \"Carrie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>W./C. 6607</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>890</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Behr, Mr. Karl Howell</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111369</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>C148</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>891</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Dooley, Mr. Patrick</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>370376</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass  \\\n",
       "0              1         0       3   \n",
       "1              2         1       1   \n",
       "2              3         1       3   \n",
       "3              4         1       1   \n",
       "4              5         0       3   \n",
       "..           ...       ...     ...   \n",
       "886          887         0       2   \n",
       "887          888         1       1   \n",
       "888          889         0       3   \n",
       "889          890         1       1   \n",
       "890          891         0       3   \n",
       "\n",
       "                                                  Name     Sex   Age  SibSp  \\\n",
       "0                              Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1    Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                               Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3         Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                             Allen, Mr. William Henry    male  35.0      0   \n",
       "..                                                 ...     ...   ...    ...   \n",
       "886                              Montvila, Rev. Juozas    male  27.0      0   \n",
       "887                       Graham, Miss. Margaret Edith  female  19.0      0   \n",
       "888           Johnston, Miss. Catherine Helen \"Carrie\"  female   NaN      1   \n",
       "889                              Behr, Mr. Karl Howell    male  26.0      0   \n",
       "890                                Dooley, Mr. Patrick    male  32.0      0   \n",
       "\n",
       "     Parch            Ticket     Fare Cabin Embarked  \n",
       "0        0         A/5 21171   7.2500   NaN        S  \n",
       "1        0          PC 17599  71.2833   C85        C  \n",
       "2        0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3        0            113803  53.1000  C123        S  \n",
       "4        0            373450   8.0500   NaN        S  \n",
       "..     ...               ...      ...   ...      ...  \n",
       "886      0            211536  13.0000   NaN        S  \n",
       "887      0            112053  30.0000   B42        S  \n",
       "888      2        W./C. 6607  23.4500   NaN        S  \n",
       "889      0            111369  30.0000  C148        C  \n",
       "890      0            370376   7.7500   NaN        Q  \n",
       "\n",
       "[891 rows x 12 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gh_path = 'https://raw.githubusercontent.com/dsindy/kaggle-titanic/master/data/'\n",
    "df_train = pd.read_csv(gh_path + 'train.csv')\n",
    "df_test = pd.read_csv(gh_path + 'test.csv')\n",
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "59rMrjvLCnLY"
   },
   "source": [
    "### Feature engineering\n",
    "Main task in this section was to prepare dataset for modeling. It was necessary to remove certain features that were redundant, that is \"Name\", \"Ticket\", \"Cabin\" and \"Embarked\". It was concluded, that name of the passenger, port of embarkation, ticket and cabin number were meaningless for analysis. Nevertheless, we concluded, that cabin placement on ship might have had influence on priority over reaching the lifeboats. Information about cabin placement could be hidden in ticket's fare,. This feature was left in dataset to include passenger placement in model.\n",
    "\n",
    "Another change that had to be made was the change of sex's values from string to binary. In the project attributes for male and female were assigned to 0 and 1 respectively.\n",
    "\n",
    "Last thing that was done, was the drop of keys with missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "XvEKGNJ6Crfu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     PassengerId  Survived  Pclass  Sex   Age  SibSp  Parch     Fare\n",
      "0              1         0       3    0  22.0      1      0   7.2500\n",
      "1              2         1       1    1  38.0      1      0  71.2833\n",
      "2              3         1       3    1  26.0      0      0   7.9250\n",
      "3              4         1       1    1  35.0      1      0  53.1000\n",
      "4              5         0       3    0  35.0      0      0   8.0500\n",
      "..           ...       ...     ...  ...   ...    ...    ...      ...\n",
      "885          886         0       3    1  39.0      0      5  29.1250\n",
      "886          887         0       2    0  27.0      0      0  13.0000\n",
      "887          888         1       1    1  19.0      0      0  30.0000\n",
      "889          890         1       1    0  26.0      0      0  30.0000\n",
      "890          891         0       3    0  32.0      0      0   7.7500\n",
      "\n",
      "[714 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "# Drop unnecessary columns\n",
    "columns_to_drop = ['Name','Ticket', 'Cabin', 'Embarked']\n",
    "df_train_processed = df_train.drop(columns=columns_to_drop)\n",
    "df_test_processed = df_test.drop(columns=columns_to_drop)\n",
    "\n",
    "# Drop keys with any NaN values\n",
    "df_train_processed = df_train_processed.dropna()\n",
    "\n",
    "# Change nan values for statisticaly correct values\n",
    "average_age = df_test_processed['Age'].mean()\n",
    "df_test_processed['Age'].fillna(average_age, inplace=True)\n",
    "\n",
    "average_fare_train = df_test_processed['Fare'].mean()\n",
    "df_test_processed['Fare'].fillna(average_fare_train, inplace=True)\n",
    "\n",
    "# Sex changed to binary\n",
    "df_train_processed['Sex'] = df_train_processed['Sex'].replace({'male': 0, 'female': 1})\n",
    "df_test_processed['Sex'] = df_test_processed['Sex'].replace({'male': 0, 'female': 1})\n",
    "\n",
    "print(df_train_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processed train dataset was next splited into training and testing sets using train_test_split functntion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df_train_processed['Survived'].values\n",
    "X = df_train_processed[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare']].values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15, random_state=420)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Perceptron\n",
    "The perceptron, a fundamental concept in artificial intelligence and machine learning, is a simple type of artificial neuron inspired by the way biological neurons work in the human brain. Developed by Frank Rosenblatt in 1957, perceptrons serve as the building blocks for more complex neural network architectures. A perceptron takes multiple input features, applies weights to these inputs, and produces an output based on a defined activation function. The weights are adjusted during training to enable the perceptron to learn patterns and make predictions.\n",
    "\n",
    "In this code, a PerceptronLogistic class is implemented to showcase a basic perceptron for logistic regression. Logistic regression is a binary classification algorithm, and the perceptron is trained using gradient descent to find optimal weights and biases. The code includes data preprocessing steps, the implementation of the perceptron class, and training and evaluation of the model on the Titanic dataset. Additionally, scikit-learn's Logistic Regression is used as a benchmark for comparison.\n",
    "\n",
    "Parameters:\n",
    "    - W (np.array): Initial weights for the perceptron.\n",
    "    - b (float): Initial bias for the perceptron.\n",
    "    - epo (int): Number of training epochs.\n",
    "    - lr (float): Learning rate for gradient descent.\n",
    "\n",
    "Methods:\n",
    "- fit(X, y, randomstate=None, batch=1, verbose=False):\n",
    "    Train the perceptron on the provided training data (X, y).\n",
    "\n",
    "    Parameters:\n",
    "    - X (np.array): Training input data.\n",
    "    - y (np.array): Target labels for training data.\n",
    "    - randomstate (int or None): Seed for random weight initialization.\n",
    "    - batch (int): Size of mini-batches for gradient descent.\n",
    "    - verbose (bool): If True, display training progress.\n",
    "\n",
    "    Returns:\n",
    "    - history (np.array): Array containing the training loss at each epoch.\n",
    "\n",
    "- predict(X):\n",
    "    Make predictions using the trained perceptron.\n",
    "\n",
    "    Parameters:\n",
    "    - X (np.array): Input data for making predictions.\n",
    "\n",
    "    Returns:\n",
    "    - predictions (np.array): Predicted probabilities.\n",
    "\n",
    "- plot_history(history, ax=None):\n",
    "    Plot the training loss history.\n",
    "\n",
    "    Parameters:\n",
    "    - history (np.array): Array containing the training loss at each epoch.\n",
    "    - ax (matplotlib.axes._subplots.AxesSubplot or None): Matplotlib axis for plotting.\n",
    "\n",
    "Notes:\n",
    "- The perceptron uses a sigmoid activation function for logistic regression.\n",
    "- Training is performed using gradient descent with an option for batch processing.\n",
    "- Implements basic methods for perceptron initialization, prediction, and visualization."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "PerceptronLogistic Class Implementation:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 323,
     "status": "ok",
     "timestamp": 1700566854641,
     "user": {
      "displayName": "Adam Kozakowski",
      "userId": "03759211285170292721"
     },
     "user_tz": -60
    },
    "id": "ND2iJXTwCwnD",
    "outputId": "4b465522-e449-4195-effd-2eb87646d420"
   },
   "outputs": [],
   "source": [
    "class PerceptronLogistic:\n",
    "    def __init__(self, W : np.array,  b = 0, epo = 100, lr = 0.01):\n",
    "        # Initialization of perceptron parameters\n",
    "        self.W = W\n",
    "        self.N = len(W)\n",
    "        self.b = b\n",
    "        self.epo = epo\n",
    "        self.lr = lr\n",
    "\n",
    "    ############################## GETTERS ##############################\n",
    "\n",
    "    def get_N(self):\n",
    "        return self.N\n",
    "    \n",
    "    def get_b(self):\n",
    "        return self.b\n",
    "    \n",
    "    def get_W(self):\n",
    "        return self.W\n",
    "    \n",
    "    ############################## SETTERS ##############################\n",
    "    \n",
    "    # Create setters for the perceptron\n",
    "    def set_N(self, N):\n",
    "        self.N = N\n",
    "        self.W = np.zeros(N)\n",
    "        \n",
    "    def set_b(self, b):\n",
    "        self.b = b\n",
    "        \n",
    "    def set_W(self, W : np.array):\n",
    "        if W.ndim != 1:\n",
    "            print(\"Cannot set such weights -> dimension wrong\")\n",
    "            return\n",
    "        self.N = W.shape[0]\n",
    "        self.W = W\n",
    "    \n",
    "    def set_lr(self, lr):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def set_epo(self, epo):\n",
    "        self.epo = epo\n",
    "\n",
    "    ############################## GETTERS OVERRIDE ##############################\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        return self.W[key]\n",
    "    \n",
    "    def __setitem__(self, key, value):\n",
    "        self.W[key] = value\n",
    "        \n",
    "    def __getslice(self, i, j):\n",
    "        return self.W[i:j]\n",
    "    \n",
    "    def get_lr(self):\n",
    "        return self.lr\n",
    "\n",
    "    def get_epo(self):\n",
    "        return self.epo\n",
    "    \n",
    "    # set the string output of the perceptron\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Am a perceptron of N={self.N} dimension{'s' if self.N > 1 else ''} biased with b={self.b}\"    \n",
    "\n",
    "    ############################## OPERATORS OVERRIDE ##############################\n",
    "     \n",
    "    def __mul__(self, other):\n",
    "        return self.activation_function(other)\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        return self.__mul__(other)\n",
    "\n",
    "    ############################## PERCEPTRON METHODS ##############################\n",
    "    \n",
    "    '''\n",
    "    Net output is the basic body action of the perceptron. On top of it, the activation function is used.\n",
    "    '''\n",
    "    def net_output(self, X):\n",
    "        # Computes the net output of the perceptron\n",
    "        return np.dot(X, self.W) + self.b\n",
    "\n",
    "    def activation_function(self, X):\n",
    "        # Applies the sigmoid activation function\n",
    "        net_output = self.net_output(X)\n",
    "        return (1.0 / (1.0 + np.exp(-np.clip(net_output, -500, 500)))).reshape(-1, 1)\n",
    "\n",
    "    def loss(self, y_true: np.array, y_pred: np.array):\n",
    "        # Computes the logistic loss\n",
    "        epsilon = 1e-15\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        return -np.sum(np.multiply(y_true, np.log(y_pred)) + np.multiply((1.0 - y_true), np.log(1.0 - y_pred)))\n",
    "\n",
    "    def gradient(self, x_true, y_true, prediction):\n",
    "        # Computes the gradient for weight and bias updates\n",
    "        val = y_true - prediction\n",
    "        suma_w = np.dot(val, x_true)\n",
    "        suma_b = np.sum(val)\n",
    "        return suma_b, suma_w\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Makes predictions using the activation function\n",
    "        return self.activation_function(X)    \n",
    "\n",
    "    def fit(self, X, y, randomstate = None, batch = 1, verbose = False):\n",
    "        # Trains the perceptron using gradient descent\n",
    "        \n",
    "        if type(X) != np.ndarray:\n",
    "            X = np.array(X)\n",
    "        if type(y) != np.ndarray:\n",
    "            y = np.array(y).reshape(-1,1)\n",
    "            \n",
    "        # give fit the parameter randomstate and whenever it is not None, the weights\n",
    "        # are reset to be random normal - this ensures random starting point of gradient descent\n",
    "        if randomstate is not None:\n",
    "            self.W = np.random.normal(0.0, 0.1, self.N)\n",
    "            self.b = np.random.normal(0.0, 1.0)\n",
    "        \n",
    "        # Save the history of the losses. Why?\n",
    "        history = []\n",
    "        # If we want to calculate the gradient in buckets (look for description of the batch)\n",
    "        bucket_num = len(X) // batch\n",
    "        # slice the data onto batches without shuffling (no stochasticity)\n",
    "        slicing = lambda x, b: x[(b-1)*batch:b*batch]\n",
    "        \n",
    "        # iterate epochs\n",
    "        for epo in range(self.epo):\n",
    "            # iterate batches\n",
    "            loss = 0.0\n",
    "            for bin in range(1, bucket_num + 1):\n",
    "                X_slice = slicing(X,bin)\n",
    "                y_slice = slicing(y,bin)\n",
    "                # predict the output for a given slice (what is the shape of the output?)\n",
    "                pred = self.predict(X_slice)\n",
    "                \n",
    "                suma_b, suma_w = self.gradient(X_slice, y_slice, pred.flatten())\n",
    "\n",
    "                # calculate loss\n",
    "                loss += self.loss(y_slice, pred.flatten())\n",
    "                \n",
    "                # update the weights\n",
    "                self.W += np.mean(suma_w) * self.lr\n",
    "                self.b += np.mean(suma_b) * self.lr\n",
    "            # calculate average loss\n",
    "            loss/=bucket_num\n",
    "\n",
    "            history.append(loss.flatten())\n",
    "        return np.array(history).flatten()\n",
    "    \n",
    "    def plot_history(self, history, ax = None):\n",
    "        # Plots the training history\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots()\n",
    "        ax.set_xlabel('epo')\n",
    "        ax.set_ylabel('loss')\n",
    "        ax.plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### PerceptronLogistic_vol_2 - A perceptron-based logistic regression model with enhancements.\n",
    "\n",
    "The ImprovedPerceptronLogistic class extends the foundational PerceptronLogistic class, introducing enhancements to boost training efficiency, convergence, and generalization. Notable improvements include the incorporation of learning rate decay, regularization, and early stopping.\n",
    "\n",
    "Learning Rate Decay:\n",
    "This enhancement involves gradually reducing the learning rate during training. A decaying learning rate helps fine-tune model adjustments, promoting smoother convergence.\n",
    "\n",
    "Early Stopping:\n",
    "To prevent overfitting and enhance generalization, early stopping is implemented. This feature halts training when the model's performance on a validation set ceases to improve, ensuring the model does not over-adapt to the training data.\n",
    "\n",
    "Regularization:\n",
    "By introducing regularization terms to the loss function, the model penalizes excessively large weights. This regularization mitigates overfitting, fostering improved generalization and robustness.\n",
    "\n",
    "Parameters:\n",
    "- W (np.array): Initial weights for the perceptron.\n",
    "- b (float): Initial bias for the perceptron.\n",
    "- epo (int): Number of training epochs.\n",
    "- lr (float): Initial learning rate for gradient descent.\n",
    "- decay_rate (float): Rate at which the learning rate decays over epochs.\n",
    "- regularization_strength (float): Strength of L2 regularization to prevent overfitting.\n",
    "\n",
    "Methods:\n",
    "- fit(X, y, randomstate=None, batch=1, verbose=False, X_val=None, y_val=None):\n",
    "    Train the perceptron on the provided training data (X, y).\n",
    "\n",
    "    Parameters:\n",
    "    - X (np.array): Training input data.\n",
    "    - y (np.array): Target labels for training data.\n",
    "    - randomstate (int or None): Seed for random weight initialization.\n",
    "    - batch (int): Size of mini-batches for gradient descent.\n",
    "    - verbose (bool): If True, display training progress.\n",
    "    - X_val (np.array or None): Validation input data.\n",
    "    - y_val (np.array or None): Target labels for validation data.\n",
    "\n",
    "Returns:\n",
    "- history (np.array): Array containing the training loss at each epoch.\n",
    "\n",
    "Notes:\n",
    "- Inherits from the PerceptronLogistic class and extends its functionality.\n",
    "- Provides options for learning rate decay, L2 regularization, and early stopping."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "outputs": [],
   "source": [
    "class PerceptronLogistic_vol_2(PerceptronLogistic):\n",
    "\n",
    "    def __init__(self, W: np.array, b=0, epo=100, lr=0.01, decay_rate=0.9, regularization_strength=0.01):\n",
    "        super().__init__(W, b, epo, lr)\n",
    "        self.decay_rate = decay_rate\n",
    "        self.regularization_strength = regularization_strength\n",
    "\n",
    "    def fit(self, X, y, randomstate=None, batch=1, verbose=False, X_val=None, y_val=None):\n",
    "        if randomstate is not None:\n",
    "            self.W = np.random.normal(0.0, 0.1, self.N)\n",
    "            self.b = np.random.normal(0.0, 1.0)\n",
    "\n",
    "        history = []\n",
    "        bucket_num = len(X) // batch\n",
    "        slicing = lambda x, b: x[(b-1)*batch:b*batch]\n",
    "\n",
    "        for epo in range(self.epo):\n",
    "            loss = 0.0\n",
    "            for bin in range(1, bucket_num + 1):\n",
    "                X_slice = slicing(X, bin)\n",
    "                y_slice = slicing(y, bin)\n",
    "                pred = self.predict(X_slice)\n",
    "\n",
    "                suma_b, suma_w = self.gradient(X_slice, y_slice, pred.flatten())\n",
    "\n",
    "                # Add regularization terms to the gradient\n",
    "                suma_w += self.regularization_strength * self.W\n",
    "                suma_b += self.regularization_strength * self.b\n",
    "\n",
    "                loss += self.loss(y_slice, pred.flatten())\n",
    "\n",
    "                self.W += np.mean(suma_w) * self.lr\n",
    "                self.b += np.mean(suma_b) * self.lr\n",
    "\n",
    "            # Learning rate decay\n",
    "            self.lr *= self.decay_rate\n",
    "\n",
    "            loss /= bucket_num\n",
    "            history.append(loss.flatten())\n",
    "\n",
    "            # Check for early stopping\n",
    "            if X_val is not None and y_val is not None:\n",
    "                val_pred = self.predict(X_val)\n",
    "                val_loss = self.loss(y_val, val_pred.flatten())\n",
    "                if len(history) > 1 and val_loss > history[-2]:\n",
    "                    print(f\"Early stopping at epoch {epo}\")\n",
    "                    break\n",
    "\n",
    "\n",
    "        return np.array(history).flatten()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "git Model Training and Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron accuracy: 0.6083916083916084\n"
     ]
    }
   ],
   "source": [
    "X = df_train_processed.drop('Survived', axis=1)\n",
    "y = df_train_processed['Survived']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "perceptron_model = PerceptronLogistic(W=np.zeros(X_train.shape[1]), b=0.0, epo=500, lr=2e-3)\n",
    "history = perceptron_model.fit(X_train_scaled, y_train, randomstate=42, batch=32, verbose=True)\n",
    "y_pred = perceptron_model.predict(X_test_scaled)\n",
    "y_pred_binary = (y_pred >= 0.7).astype(int)\n",
    "accuracy = np.mean(y_pred_binary.flatten() == y_test)\n",
    "print(f\"Perceptron accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron accuracy: 0.6293706293706294\n"
     ]
    }
   ],
   "source": [
    "improved_perceptron_model = PerceptronLogistic_vol_2(W=np.zeros(X_train.shape[1]), b=0.0, epo=500, lr=2e-3, decay_rate=0.9, regularization_strength=0.01)\n",
    "history = improved_perceptron_model.fit(X_train_scaled, y_train, randomstate=42, batch=32, verbose=True)\n",
    "y_pred = improved_perceptron_model.predict(X_test_scaled)\n",
    "y_pred_binary = (y_pred >= 0.7).astype(int)\n",
    "accuracy = np.mean(y_pred_binary.flatten() == y_test)\n",
    "print(f\"Perceptron accuracy: {accuracy}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "Usage scikit-learn's Logistic Regression as a benchmark"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scikit-learn Logistic Regression Accuracy: 0.7552447552447552\n"
     ]
    }
   ],
   "source": [
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "lr_accuracy = lr_model.score(X_test_scaled, y_test)\n",
    "print(f\"Scikit-learn Logistic Regression Accuracy: {lr_accuracy}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4YxOo4SC0mr"
   },
   "source": [
    "### Suport Vector Machine\n",
    "\n",
    "Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks. It is commonly used in classification problems where occur non-linear decision boundaries.  It aims to find the optimal hyperplane that separates different classes in the input space. In project, vectors are in 6-th dimention (described by 6 values: Pclass, Sex, Age, SibSp, Parch and Fare). Support vectors are data points from each class that are closest to the hyperplane. They play a crucial role in determining the optimal hyperplane and maximizing the margin between classes - the distance between the hyperplane and the nearest data point from each class.\n",
    "\n",
    "There was unsuccesfull attempt to implement SVM class from scratch. A Decision was made to implement model using only scikit-learn package functions. Nevertheless, different kernel functions were used: linear, polynomial, radial basis function (RBF) and sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of linear kernel SVM: 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "### linear kernel\n",
    "svm_model = SVC(kernel='linear')  #try different kernels\n",
    "svm_model.fit(X_train, Y_train)\n",
    "\n",
    "# making predictions on the test set\n",
    "predictions = svm_model.predict(X_test)\n",
    "\n",
    "# evaluating accuracies\n",
    "accuracy = svm_model.score(X_test, Y_test)\n",
    "print(f\"Accuracy of linear kernel SVM: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of poly kernel SVM: 0.6388888888888888\n"
     ]
    }
   ],
   "source": [
    "### poly kernel\n",
    "svm_model = SVC(kernel='poly')  #try different kernels\n",
    "svm_model.fit(X_train, Y_train)\n",
    "predictions = svm_model.predict(X_test)\n",
    "accuracy = svm_model.score(X_test, Y_test)\n",
    "print(f\"Accuracy of poly kernel SVM: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Radial Basis Function kernel SVM: 0.6574074074074074\n"
     ]
    }
   ],
   "source": [
    "### Radial Basis Function kernel\n",
    "svm_model = SVC(kernel='rbf')  #try different kernels\n",
    "svm_model.fit(X_train, Y_train)\n",
    "predictions = svm_model.predict(X_test)\n",
    "accuracy = svm_model.score(X_test, Y_test)\n",
    "print(f\"Accuracy of Radial Basis Function kernel SVM: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of sigmoid kernel SVM: 0.6203703703703703\n"
     ]
    }
   ],
   "source": [
    "### sigmoid kernel\n",
    "svm_model = SVC(kernel='sigmoid')  #try different kernels\n",
    "svm_model.fit(X_train, Y_train)\n",
    "predictions = svm_model.predict(X_test)\n",
    "accuracy = svm_model.score(X_test, Y_test)\n",
    "print(f\"Accuracy of sigmoid kernel SVM: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best accuracy was reached for linear kernel SVM, and it was used for final prediction for Kaggle competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     PassengerId  Pclass  Sex       Age  SibSp  Parch      Fare\n",
      "0            892       3    0  34.50000      0      0    7.8292\n",
      "1            893       3    1  47.00000      1      0    7.0000\n",
      "2            894       2    0  62.00000      0      0    9.6875\n",
      "3            895       3    0  27.00000      0      0    8.6625\n",
      "4            896       3    1  22.00000      1      1   12.2875\n",
      "..           ...     ...  ...       ...    ...    ...       ...\n",
      "413         1305       3    0  30.27259      0      0    8.0500\n",
      "414         1306       1    1  39.00000      0      0  108.9000\n",
      "415         1307       3    0  38.50000      0      0    7.2500\n",
      "416         1308       3    0  30.27259      0      0    8.0500\n",
      "417         1309       3    0  30.27259      1      1   22.3583\n",
      "\n",
      "[418 rows x 7 columns]\n",
      "\n",
      "NaN rows in test set:\n",
      "Empty DataFrame\n",
      "Columns: [PassengerId, Pclass, Name, Sex, Age, SibSp, Parch, Ticket, Fare, Cabin, Embarked]\n",
      "Index: []\n",
      "Your submission was successfully saved!\n"
     ]
    }
   ],
   "source": [
    "# save the results\n",
    "predictions = svm_model.predict(df_test_processed[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare']].values)\n",
    "output = pd.DataFrame({'PassengerId': df_test_processed.PassengerId, 'Survived': predictions})\n",
    "output.to_csv('SVM_kaggle_submission.csv', index=False)\n",
    "print(\"Your submission was successfully saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "efv0crrNC7Aw"
   },
   "source": [
    "#### Random Forest Model\n",
    "##### To do:\n",
    "- arono"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random forest (kaggle example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Random Forest Classifier: 0.8159371492704826\n",
      "Your submission was successfully saved!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "test_data = df_test\n",
    "y = df_train[\"Survived\"]\n",
    "\n",
    "features = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\n",
    "X = pd.get_dummies(df_train[features])\n",
    "X_test = pd.get_dummies(test_data[features])\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\n",
    "model.fit(X, y)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "accuracy = model.score(X, y)\n",
    "print(f\"Accuracy of Random Forest Classifier: {accuracy}\")\n",
    "\n",
    "output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\n",
    "output.to_csv('RF_kaggle_submission.csv', index=False)\n",
    "print(\"Your submission was successfully saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UaJFk54ICW0c"
   },
   "source": [
    "#### Analysis of results\n",
    "\n",
    "JAK PCA TO MOŻNA BIBLIOTEK UŻYĆ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "To do:\n",
    "- did we reachedour goals?\n",
    "- what is interesting in our solutions\n",
    "- what could be done better? limits and possible improvement"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
