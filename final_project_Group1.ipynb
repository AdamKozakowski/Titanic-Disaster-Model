{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1S8EghnyjxYg"
   },
   "source": [
    "# Titanic disaster model"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Inrotuction\n",
    " The sinking of the Titanic is a well-known disaster in history. On April 15, 1912, during its first journey, the supposedly unsinkable RMS Titanic hit an iceberg and sank. Sadly, there weren't sufficient lifeboats for everyone on board, leading to the loss of 1502 lives out of 2224 passengers and crew. The RMS Titanic was the largest ship afloat at the time it entered service and was the second of three Olympic-class ocean liners operated by the White Star Line. The Titanic was built by the Harland and Wolff shipyard in Belfast. Thomas Andrews, her architect, died in the disaster. Survival was partly a matter of luck, but certain groups had better chances of making it through than others. In our project, we want to build a predictive model that answers the question: *\"what sorts of people were more likely to survive?\"* using passenger data [1]."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Machine learning is increasingly being used to predict survival outcomes, including in scenarios like the Titanic disaster.\n",
    "Common methods used for predicting survival:\n",
    "\n",
    "- **Logistic Regression [2]**:\n",
    "Classic statistical method employed for binary classification problems, such as predicting survival or non-survival.\n",
    "It models the probability of an event occurring (e.g., survival) based on one or more predictor variables. A perceptron is a single layer neural network\n",
    "\n",
    "- **Random Forests [3]**:\n",
    "Commonly-used machine learning algorithm trademarked by Leo Breiman and Adele Cutler, which combines the output of multiple decision trees to reach a single resultThis method is effective in capturing complex relationships in the data and is resistant to overfitting.\n",
    "\n",
    "- **Support Vector Machines (SVM) [4]**:\n",
    "Supervised learning algorithm that can be used for classification tasks.\n",
    "It works well for both linear and non-linear relationships in the data and can be applied to predict survival based on various features. The objective of the support vector machine algorithm is to find a hyperplane in an N-dimensional space(N — the number of features) that distinctly classifies the data points.\n",
    "\n",
    "- **Deep learning models [5]**:\n",
    "subset of machine learning, which is essentially a neural network with three or more layers. These neural networks attempt to simulate the behavior of the human brain—albeit far from matching its ability—allowing it to “learn” from large amounts of data. While a neural network with a single layer can still make approximate predictions, additional hidden layers can help to optimize and refine for accuracy. They can be used to create complex models for predicting survival, provided there is enough data available.\n",
    "\n",
    "- **Feature Engineering [6]**:\n",
    "Feature engineering is the process of selecting, manipulating, and transforming raw data into features that can be used in supervised learning. It can produce new features for both supervised and unsupervised learning, with the goal of simplifying and speeding up data transformations while also enhancing model accuracy. Feature engineering is required when working with machine learning models.\n",
    "\n",
    "In our prediction, we will use and compare with each other single perceptron model, as an example of linear regression, SVM and Random Tree model. Our expectation is to predict if the passenger would survive or not with at least 60% accuracy. We will also compare our models with the ones, provided by the sklearn learn library"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/competitions/titanic\n",
    "https://www.kaggle.com/code/alexisbcook/titanic-tutorial/notebook\n",
    "[1] https://towardsdatascience.com/predicting-the-survival-of-titanic-passengers-30870ccc7e8\n",
    "[2] https://towardsdatascience.com/the-perceptron-3af34c84838c\n",
    "[3] https://www.ibm.com/topics/random-forest\n",
    "[4] https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47\n",
    "[5] https://www.ibm.com/topics/deep-learning\n",
    "[6] https://towardsdatascience.com/what-is-feature-engineering-importance-tools-and-techniques-for-machine-learning-2080b0269f10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zxNBdgCNCZSn"
   },
   "source": [
    "### Seting up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "from scipy import optimize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.optimize import minimize, LinearConstraint\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the dataset\n",
    "The dataset used in the project was downloaded from github repository provided by Kaggle: https://raw.githubusercontent.com/dsindy/kaggle-titanic/master/data/.\n",
    "The training-set has 891 examples and 11 features + the target variable (survived). It were as fallows:\n",
    "\n",
    "PassengerId - unigue id of a passenger,\n",
    "Survival - Survival result (0 = No, 1 = Yes),\n",
    "Pclass\t- ticket class\t(1 = 1st, 2 = 2nd, 3 = 3rd),\n",
    "Name - passenger name,\n",
    "Sex     - sex,\n",
    "Age\t    -     age in years,\n",
    "Sibsp\t-    number of siblings / spouses aboard the Titanic, \t\n",
    "Parch\t-     number of parents / children aboard the Titanic\t\n",
    "Ticket\t-    ticket number, \n",
    "Fare\t-    passenger fare\t\n",
    "Cabin\t-    cabin number\t\n",
    "Embarked -\tport of Embarkation\tC = Cherbourg, Q = Queenstown, S = Southampton.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 666
    },
    "executionInfo": {
     "elapsed": 263,
     "status": "ok",
     "timestamp": 1700692440573,
     "user": {
      "displayName": "Kirill Korzuk",
      "userId": "04732590717018364342"
     },
     "user_tz": -180
    },
    "id": "C0ZWTfkFj8XM",
    "outputId": "5c778832-e116-4fb5-96f0-eef0058f510c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "     PassengerId  Survived  Pclass  \\\n0              1         0       3   \n1              2         1       1   \n2              3         1       3   \n3              4         1       1   \n4              5         0       3   \n..           ...       ...     ...   \n886          887         0       2   \n887          888         1       1   \n888          889         0       3   \n889          890         1       1   \n890          891         0       3   \n\n                                                  Name     Sex   Age  SibSp  \\\n0                              Braund, Mr. Owen Harris    male  22.0      1   \n1    Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n2                               Heikkinen, Miss. Laina  female  26.0      0   \n3         Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n4                             Allen, Mr. William Henry    male  35.0      0   \n..                                                 ...     ...   ...    ...   \n886                              Montvila, Rev. Juozas    male  27.0      0   \n887                       Graham, Miss. Margaret Edith  female  19.0      0   \n888           Johnston, Miss. Catherine Helen \"Carrie\"  female   NaN      1   \n889                              Behr, Mr. Karl Howell    male  26.0      0   \n890                                Dooley, Mr. Patrick    male  32.0      0   \n\n     Parch            Ticket     Fare Cabin Embarked  \n0        0         A/5 21171   7.2500   NaN        S  \n1        0          PC 17599  71.2833   C85        C  \n2        0  STON/O2. 3101282   7.9250   NaN        S  \n3        0            113803  53.1000  C123        S  \n4        0            373450   8.0500   NaN        S  \n..     ...               ...      ...   ...      ...  \n886      0            211536  13.0000   NaN        S  \n887      0            112053  30.0000   B42        S  \n888      2        W./C. 6607  23.4500   NaN        S  \n889      0            111369  30.0000  C148        C  \n890      0            370376   7.7500   NaN        Q  \n\n[891 rows x 12 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Braund, Mr. Owen Harris</td>\n      <td>male</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>A/5 21171</td>\n      <td>7.2500</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n      <td>female</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>PC 17599</td>\n      <td>71.2833</td>\n      <td>C85</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>3</td>\n      <td>Heikkinen, Miss. Laina</td>\n      <td>female</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>STON/O2. 3101282</td>\n      <td>7.9250</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n      <td>female</td>\n      <td>35.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>113803</td>\n      <td>53.1000</td>\n      <td>C123</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Allen, Mr. William Henry</td>\n      <td>male</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>373450</td>\n      <td>8.0500</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>886</th>\n      <td>887</td>\n      <td>0</td>\n      <td>2</td>\n      <td>Montvila, Rev. Juozas</td>\n      <td>male</td>\n      <td>27.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>211536</td>\n      <td>13.0000</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>887</th>\n      <td>888</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Graham, Miss. Margaret Edith</td>\n      <td>female</td>\n      <td>19.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>112053</td>\n      <td>30.0000</td>\n      <td>B42</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>888</th>\n      <td>889</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Johnston, Miss. Catherine Helen \"Carrie\"</td>\n      <td>female</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>2</td>\n      <td>W./C. 6607</td>\n      <td>23.4500</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>889</th>\n      <td>890</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Behr, Mr. Karl Howell</td>\n      <td>male</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>111369</td>\n      <td>30.0000</td>\n      <td>C148</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>890</th>\n      <td>891</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Dooley, Mr. Patrick</td>\n      <td>male</td>\n      <td>32.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>370376</td>\n      <td>7.7500</td>\n      <td>NaN</td>\n      <td>Q</td>\n    </tr>\n  </tbody>\n</table>\n<p>891 rows × 12 columns</p>\n</div>"
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gh_path = 'https://raw.githubusercontent.com/dsindy/kaggle-titanic/master/data/'\n",
    "df_train = pd.read_csv(gh_path + 'train.csv')\n",
    "df_test = pd.read_csv(gh_path + 'test.csv')\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "outputs": [
    {
     "data": {
      "text/plain": "       PassengerId    Survived      Pclass         Age       SibSp  \\\ncount   891.000000  891.000000  891.000000  714.000000  891.000000   \nmean    446.000000    0.383838    2.308642   29.699118    0.523008   \nstd     257.353842    0.486592    0.836071   14.526497    1.102743   \nmin       1.000000    0.000000    1.000000    0.420000    0.000000   \n25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n75%     668.500000    1.000000    3.000000   38.000000    1.000000   \nmax     891.000000    1.000000    3.000000   80.000000    8.000000   \n\n            Parch        Fare  \ncount  891.000000  891.000000  \nmean     0.381594   32.204208  \nstd      0.806057   49.693429  \nmin      0.000000    0.000000  \n25%      0.000000    7.910400  \n50%      0.000000   14.454200  \n75%      0.000000   31.000000  \nmax      6.000000  512.329200  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Fare</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>891.000000</td>\n      <td>891.000000</td>\n      <td>891.000000</td>\n      <td>714.000000</td>\n      <td>891.000000</td>\n      <td>891.000000</td>\n      <td>891.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>446.000000</td>\n      <td>0.383838</td>\n      <td>2.308642</td>\n      <td>29.699118</td>\n      <td>0.523008</td>\n      <td>0.381594</td>\n      <td>32.204208</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>257.353842</td>\n      <td>0.486592</td>\n      <td>0.836071</td>\n      <td>14.526497</td>\n      <td>1.102743</td>\n      <td>0.806057</td>\n      <td>49.693429</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.420000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>223.500000</td>\n      <td>0.000000</td>\n      <td>2.000000</td>\n      <td>20.125000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>7.910400</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>446.000000</td>\n      <td>0.000000</td>\n      <td>3.000000</td>\n      <td>28.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>14.454200</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>668.500000</td>\n      <td>1.000000</td>\n      <td>3.000000</td>\n      <td>38.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>31.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>891.000000</td>\n      <td>1.000000</td>\n      <td>3.000000</td>\n      <td>80.000000</td>\n      <td>8.000000</td>\n      <td>6.000000</td>\n      <td>512.329200</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Above we can see that 38% out of the training-set survived the Titanic."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "59rMrjvLCnLY"
   },
   "source": [
    "### Feature engineering\n",
    "Main task in this section was to prepare dataset for modeling. It was necessary to remove certain features that were redundant, that is \"Name\", \"Ticket\", \"Cabin\" and \"Embarked\". It was concluded, that name of the passenger, port of embarkation, ticket and cabin number were meaningless for analysis. Nevertheless, we concluded, that cabin placement on ship might have had influence on priority over reaching the lifeboats. Information about cabin placement could be hidden in ticket's fare,. This feature was left in dataset to include passenger placement in model.\n",
    "\n",
    "Another change that had to be made was the change of sex's values from string to binary. In the project attributes for male and female were assigned to 0 and 1 respectively.\n",
    "\n",
    "Last thing that was done, was the drop of keys with missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "id": "XvEKGNJ6Crfu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     PassengerId  Survived  Pclass  Sex   Age  SibSp  Parch     Fare\n",
      "0              1         0       3    0  22.0      1      0   7.2500\n",
      "1              2         1       1    1  38.0      1      0  71.2833\n",
      "2              3         1       3    1  26.0      0      0   7.9250\n",
      "3              4         1       1    1  35.0      1      0  53.1000\n",
      "4              5         0       3    0  35.0      0      0   8.0500\n",
      "..           ...       ...     ...  ...   ...    ...    ...      ...\n",
      "885          886         0       3    1  39.0      0      5  29.1250\n",
      "886          887         0       2    0  27.0      0      0  13.0000\n",
      "887          888         1       1    1  19.0      0      0  30.0000\n",
      "889          890         1       1    0  26.0      0      0  30.0000\n",
      "890          891         0       3    0  32.0      0      0   7.7500\n",
      "\n",
      "[714 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "# Drop unnecessary columns\n",
    "columns_to_drop = ['Name','Ticket', 'Cabin', 'Embarked']\n",
    "df_train_processed = df_train.drop(columns=columns_to_drop)\n",
    "df_test_processed = df_test.drop(columns=columns_to_drop)\n",
    "\n",
    "# Drop keys with any NaN values\n",
    "df_train_processed = df_train_processed.dropna()\n",
    "\n",
    "# Change nan values for statisticaly correct values\n",
    "average_age = df_test_processed['Age'].mean()\n",
    "df_test_processed['Age'].fillna(average_age, inplace=True)\n",
    "\n",
    "average_fare_train = df_test_processed['Fare'].mean()\n",
    "df_test_processed['Fare'].fillna(average_fare_train, inplace=True)\n",
    "\n",
    "# Sex changed to binary\n",
    "df_train_processed['Sex'] = df_train_processed['Sex'].replace({'male': 0, 'female': 1})\n",
    "df_test_processed['Sex'] = df_test_processed['Sex'].replace({'male': 0, 'female': 1})\n",
    "\n",
    "print(df_train_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processed train dataset was next splited into training and testing sets using train_test_split functntion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df_train_processed['Survived'].values\n",
    "X = df_train_processed[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare']].values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15, random_state=420)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Perceptron\n",
    "The perceptron, a fundamental concept in artificial intelligence and machine learning, is a simple type of artificial neuron inspired by the way biological neurons work in the human brain. Developed by Frank Rosenblatt in 1957, perceptrons serve as the building blocks for more complex neural network architectures. A perceptron takes multiple input features, applies weights to these inputs, and produces an output based on a defined activation function. The weights are adjusted during training to enable the perceptron to learn patterns and make predictions.\n",
    "\n",
    "In this code, a PerceptronLogistic class is implemented to showcase a basic perceptron for logistic regression. Logistic regression is a binary classification algorithm, and the perceptron is trained using gradient descent to find optimal weights and biases. The code includes data preprocessing steps, the implementation of the perceptron class, and training and evaluation of the model on the Titanic dataset. Additionally, scikit-learn's Logistic Regression is used as a benchmark for comparison.\n",
    "\n",
    "Parameters:\n",
    "    - W (np.array): Initial weights for the perceptron.\n",
    "    - b (float): Initial bias for the perceptron.\n",
    "    - epo (int): Number of training epochs.\n",
    "    - lr (float): Learning rate for gradient descent.\n",
    "\n",
    "Methods:\n",
    "- fit(X, y, randomstate=None, batch=1, verbose=False):\n",
    "    Train the perceptron on the provided training data (X, y).\n",
    "\n",
    "    Parameters:\n",
    "    - X (np.array): Training input data.\n",
    "    - y (np.array): Target labels for training data.\n",
    "    - randomstate (int or None): Seed for random weight initialization.\n",
    "    - batch (int): Size of mini-batches for gradient descent.\n",
    "    - verbose (bool): If True, display training progress.\n",
    "\n",
    "    Returns:\n",
    "    - history (np.array): Array containing the training loss at each epoch.\n",
    "\n",
    "- predict(X):\n",
    "    Make predictions using the trained perceptron.\n",
    "\n",
    "    Parameters:\n",
    "    - X (np.array): Input data for making predictions.\n",
    "\n",
    "    Returns:\n",
    "    - predictions (np.array): Predicted probabilities.\n",
    "\n",
    "- plot_history(history, ax=None):\n",
    "    Plot the training loss history.\n",
    "\n",
    "    Parameters:\n",
    "    - history (np.array): Array containing the training loss at each epoch.\n",
    "    - ax (matplotlib.axes._subplots.AxesSubplot or None): Matplotlib axis for plotting.\n",
    "\n",
    "Notes:\n",
    "- The perceptron uses a sigmoid activation function for logistic regression.\n",
    "- Training is performed using gradient descent with an option for batch processing.\n",
    "- Implements basic methods for perceptron initialization, prediction, and visualization."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "PerceptronLogistic Class Implementation:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 323,
     "status": "ok",
     "timestamp": 1700566854641,
     "user": {
      "displayName": "Adam Kozakowski",
      "userId": "03759211285170292721"
     },
     "user_tz": -60
    },
    "id": "ND2iJXTwCwnD",
    "outputId": "4b465522-e449-4195-effd-2eb87646d420"
   },
   "outputs": [],
   "source": [
    "class PerceptronLogistic:\n",
    "    def __init__(self, W : np.array,  b = 0, epo = 100, lr = 0.01):\n",
    "        # Initialization of perceptron parameters\n",
    "        self.W = W\n",
    "        self.N = len(W)\n",
    "        self.b = b\n",
    "        self.epo = epo\n",
    "        self.lr = lr\n",
    "\n",
    "    ############################## GETTERS ##############################\n",
    "\n",
    "    def get_N(self):\n",
    "        return self.N\n",
    "    \n",
    "    def get_b(self):\n",
    "        return self.b\n",
    "    \n",
    "    def get_W(self):\n",
    "        return self.W\n",
    "    \n",
    "    ############################## SETTERS ##############################\n",
    "    \n",
    "    # Create setters for the perceptron\n",
    "    def set_N(self, N):\n",
    "        self.N = N\n",
    "        self.W = np.zeros(N)\n",
    "        \n",
    "    def set_b(self, b):\n",
    "        self.b = b\n",
    "        \n",
    "    def set_W(self, W : np.array):\n",
    "        if W.ndim != 1:\n",
    "            print(\"Cannot set such weights -> dimension wrong\")\n",
    "            return\n",
    "        self.N = W.shape[0]\n",
    "        self.W = W\n",
    "    \n",
    "    def set_lr(self, lr):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def set_epo(self, epo):\n",
    "        self.epo = epo\n",
    "\n",
    "    ############################## GETTERS OVERRIDE ##############################\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        return self.W[key]\n",
    "    \n",
    "    def __setitem__(self, key, value):\n",
    "        self.W[key] = value\n",
    "        \n",
    "    def __getslice(self, i, j):\n",
    "        return self.W[i:j]\n",
    "    \n",
    "    def get_lr(self):\n",
    "        return self.lr\n",
    "\n",
    "    def get_epo(self):\n",
    "        return self.epo\n",
    "    \n",
    "    # set the string output of the perceptron\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Am a perceptron of N={self.N} dimension{'s' if self.N > 1 else ''} biased with b={self.b}\"    \n",
    "\n",
    "    ############################## OPERATORS OVERRIDE ##############################\n",
    "     \n",
    "    def __mul__(self, other):\n",
    "        return self.activation_function(other)\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        return self.__mul__(other)\n",
    "\n",
    "    ############################## PERCEPTRON METHODS ##############################\n",
    "    \n",
    "    '''\n",
    "    Net output is the basic body action of the perceptron. On top of it, the activation function is used.\n",
    "    '''\n",
    "    def net_output(self, X):\n",
    "        # Computes the net output of the perceptron\n",
    "        return np.dot(X, self.W) + self.b\n",
    "\n",
    "    def activation_function(self, X):\n",
    "        # Applies the sigmoid activation function\n",
    "        net_output = self.net_output(X)\n",
    "        return (1.0 / (1.0 + np.exp(-np.clip(net_output, -500, 500)))).reshape(-1, 1)\n",
    "\n",
    "    def loss(self, y_true: np.array, y_pred: np.array):\n",
    "        # Computes the logistic loss\n",
    "        epsilon = 1e-15\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        return -np.sum(np.multiply(y_true, np.log(y_pred)) + np.multiply((1.0 - y_true), np.log(1.0 - y_pred)))\n",
    "\n",
    "    def gradient(self, x_true, y_true, prediction):\n",
    "        # Computes the gradient for weight and bias updates\n",
    "        val = y_true - prediction\n",
    "        suma_w = np.dot(val, x_true)\n",
    "        suma_b = np.sum(val)\n",
    "        return suma_b, suma_w\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Makes predictions using the activation function\n",
    "        predictions = self.activation_function(X)\n",
    "        return predictions.flatten()\n",
    "\n",
    "    def fit(self, X, y, randomstate = None, batch = 1, verbose = False):\n",
    "        # Trains the perceptron using gradient descent\n",
    "        \n",
    "        if type(X) != np.ndarray:\n",
    "            X = np.array(X)\n",
    "        if type(y) != np.ndarray:\n",
    "            y = np.array(y).reshape(-1,1)\n",
    "            \n",
    "        # give fit the parameter randomstate and whenever it is not None, the weights\n",
    "        # are reset to be random normal - this ensures random starting point of gradient descent\n",
    "        if randomstate is not None:\n",
    "            self.W = np.random.normal(0.0, 0.1, self.N)\n",
    "            self.b = np.random.normal(0.0, 1.0)\n",
    "        \n",
    "        # Save the history of the losses. Why?\n",
    "        history = []\n",
    "        # If we want to calculate the gradient in buckets (look for description of the batch)\n",
    "        bucket_num = len(X) // batch\n",
    "        # slice the data onto batches without shuffling (no stochasticity)\n",
    "        slicing = lambda x, b: x[(b-1)*batch:b*batch]\n",
    "        \n",
    "        # iterate epochs\n",
    "        for epo in range(self.epo):\n",
    "            # iterate batches\n",
    "            loss = 0.0\n",
    "            for bin in range(1, bucket_num + 1):\n",
    "                X_slice = slicing(X,bin)\n",
    "                y_slice = slicing(y,bin)\n",
    "                # predict the output for a given slice (what is the shape of the output?)\n",
    "                pred = self.predict(X_slice)\n",
    "                \n",
    "                suma_b, suma_w = self.gradient(X_slice, y_slice, pred.flatten())\n",
    "\n",
    "                # calculate loss\n",
    "                loss += self.loss(y_slice, pred.flatten())\n",
    "                \n",
    "                # update the weights\n",
    "                self.W += np.mean(suma_w) * self.lr\n",
    "                self.b += np.mean(suma_b) * self.lr\n",
    "            # calculate average loss\n",
    "            loss/=bucket_num\n",
    "\n",
    "            history.append(loss.flatten())\n",
    "        return np.array(history).flatten()\n",
    "    \n",
    "    def plot_history(self, history, ax = None):\n",
    "        # Plots the training history\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots()\n",
    "        ax.set_xlabel('epo')\n",
    "        ax.set_ylabel('loss')\n",
    "        ax.plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### PerceptronLogistic_vol_2 - A perceptron-based logistic regression model with enhancements.\n",
    "\n",
    "The PerceptronLogistic_vol_2 class extends the foundational PerceptronLogistic class, introducing enhancements to boost training efficiency, convergence, and generalization. Notable improvements include the incorporation of learning rate decay, regularization, and early stopping.\n",
    "\n",
    "Learning Rate Decay:\n",
    "This enhancement involves gradually reducing the learning rate during training. A decaying learning rate helps fine-tune model adjustments, promoting smoother convergence.\n",
    "\n",
    "Early Stopping:\n",
    "To prevent overfitting and enhance generalization, early stopping is implemented. This feature halts training when the model's performance on a validation set ceases to improve, ensuring the model does not over-adapt to the training data.\n",
    "\n",
    "Regularization:\n",
    "By introducing regularization terms to the loss function, the model penalizes excessively large weights. This regularization mitigates overfitting, fostering improved generalization and robustness.\n",
    "\n",
    "Parameters:\n",
    "- W (np.array): Initial weights for the perceptron.\n",
    "- b (float): Initial bias for the perceptron.\n",
    "- epo (int): Number of training epochs.\n",
    "- lr (float): Initial learning rate for gradient descent.\n",
    "- decay_rate (float): Rate at which the learning rate decays over epochs.\n",
    "- regularization_strength (float): Strength of L2 regularization to prevent overfitting.\n",
    "\n",
    "Methods:\n",
    "- fit(X, y, randomstate=None, batch=1, verbose=False, X_val=None, y_val=None):\n",
    "    Train the perceptron on the provided training data (X, y).\n",
    "\n",
    "    Parameters:\n",
    "    - X (np.array): Training input data.\n",
    "    - y (np.array): Target labels for training data.\n",
    "    - randomstate (int or None): Seed for random weight initialization.\n",
    "    - batch (int): Size of mini-batches for gradient descent.\n",
    "    - verbose (bool): If True, display training progress.\n",
    "    - X_val (np.array or None): Validation input data.\n",
    "    - y_val (np.array or None): Target labels for validation data.\n",
    "\n",
    "Returns:\n",
    "- history (np.array): Array containing the training loss at each epoch.\n",
    "\n",
    "Notes:\n",
    "- Inherits from the PerceptronLogistic class and extends its functionality.\n",
    "- Provides options for learning rate decay, L2 regularization, and early stopping."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "outputs": [],
   "source": [
    "class PerceptronLogistic_vol_2(PerceptronLogistic):\n",
    "\n",
    "    def __init__(self, W: np.array, b=0, epo=100, lr=0.01, decay_rate=0.9, regularization_strength=0.01):\n",
    "        super().__init__(W, b, epo, lr)\n",
    "        self.decay_rate = decay_rate\n",
    "        self.regularization_strength = regularization_strength\n",
    "\n",
    "    def fit(self, X, y, randomstate=None, batch=1, verbose=False, X_val=None, y_val=None):\n",
    "        if randomstate is not None:\n",
    "            self.W = np.random.normal(0.0, 0.1, self.N)\n",
    "            self.b = np.random.normal(0.0, 1.0)\n",
    "\n",
    "        history = []\n",
    "        bucket_num = len(X) // batch\n",
    "        slicing = lambda x, b: x[(b-1)*batch:b*batch]\n",
    "\n",
    "        for epo in range(self.epo):\n",
    "            loss = 0.0\n",
    "            for bin in range(1, bucket_num + 1):\n",
    "                X_slice = slicing(X, bin)\n",
    "                y_slice = slicing(y, bin)\n",
    "                pred = self.predict(X_slice)\n",
    "\n",
    "                suma_b, suma_w = self.gradient(X_slice, y_slice, pred.flatten())\n",
    "\n",
    "                # Add regularization terms to the gradient\n",
    "                suma_w += self.regularization_strength * self.W\n",
    "                suma_b += self.regularization_strength * self.b\n",
    "\n",
    "                loss += self.loss(y_slice, pred.flatten())\n",
    "\n",
    "                self.W += np.mean(suma_w) * self.lr\n",
    "                self.b += np.mean(suma_b) * self.lr\n",
    "\n",
    "            # Learning rate decay\n",
    "            self.lr *= self.decay_rate\n",
    "\n",
    "            loss /= bucket_num\n",
    "            history.append(loss.flatten())\n",
    "\n",
    "            # Check for early stopping\n",
    "            if X_val is not None and y_val is not None:\n",
    "                val_pred = self.predict(X_val)\n",
    "                val_loss = self.loss(y_val, val_pred.flatten())\n",
    "                if len(history) > 1 and val_loss > history[-2]:\n",
    "                    print(f\"Early stopping at epoch {epo}\")\n",
    "                    break\n",
    "\n",
    "\n",
    "        return np.array(history).flatten()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "git Model Training and Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train_processed.drop('Survived', axis=1)\n",
    "y = df_train_processed['Survived']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron accuracy: 0.5454545454545454\n"
     ]
    }
   ],
   "source": [
    "perceptron_model = PerceptronLogistic(W=np.zeros(X_train.shape[1]), b=0.0, epo=500, lr=2e-3)\n",
    "history = perceptron_model.fit(X_train_scaled, y_train, randomstate=42, batch=32, verbose=True)\n",
    "y_pred = perceptron_model.predict(X_test_scaled)\n",
    "y_pred_binary = (y_pred >= 0.5).astype(int)\n",
    "accuracy = np.mean(y_pred_binary.flatten() == y_test)\n",
    "print(f\"Perceptron accuracy: {accuracy}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your submission was successfully saved!\n"
     ]
    }
   ],
   "source": [
    "X_final_test_scaled = scaler.transform(df_test_processed)\n",
    "predictions = perceptron_model.predict(X_final_test_scaled)\n",
    "predictions_binary = (predictions >= 0.5).astype(int)\n",
    "output = pd.DataFrame({'PassengerId': df_test_processed.PassengerId.values, 'Survived': predictions_binary})\n",
    "output.to_csv('PerceptronLogistic_kaggle_submission.csv', index=False)\n",
    "print(\"Your submission was successfully saved!\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron accuracy: 0.5874125874125874\n"
     ]
    }
   ],
   "source": [
    "improved_perceptron_model = PerceptronLogistic_vol_2(W=np.zeros(X_train.shape[1]), b=0.0, epo=500, lr=2e-3, decay_rate=0.9, regularization_strength=0.01)\n",
    "history = improved_perceptron_model.fit(X_train_scaled, y_train, randomstate=42, batch=32, verbose=True)\n",
    "y_pred = improved_perceptron_model.predict(X_test_scaled)\n",
    "y_pred_binary = (y_pred >= 0.5).astype(int)\n",
    "accuracy = np.mean(y_pred_binary.flatten() == y_test)\n",
    "print(f\"Perceptron accuracy: {accuracy}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your submission was successfully saved!\n"
     ]
    }
   ],
   "source": [
    "X_final_test_scaled = scaler.transform(df_test_processed)\n",
    "predictions = perceptron_model.predict(X_final_test_scaled)\n",
    "predictions_binary = (predictions >= 0.5).astype(int)\n",
    "output = pd.DataFrame({'PassengerId': df_test_processed.PassengerId.values, 'Survived': predictions_binary})\n",
    "output.to_csv('PerceptronLogistic_vol_2_kaggle_submission.csv', index=False)\n",
    "print(\"Your submission was successfully saved!\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "Usage scikit-learn's Logistic Regression as a benchmark"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scikit-learn Logistic Regression Accuracy: 0.7552447552447552\n"
     ]
    }
   ],
   "source": [
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "lr_accuracy = lr_model.score(X_test_scaled, y_test)\n",
    "print(f\"Scikit-learn Logistic Regression Accuracy: {lr_accuracy}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4YxOo4SC0mr"
   },
   "source": [
    "### Suport Vector Machine\n",
    "\n",
    "Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks. It is commonly used in classification problems where occur non-linear decision boundaries.  It aims to find the optimal hyperplane that separates different classes in the input space. In project, vectors are in 6-th dimention (described by 6 values: Pclass, Sex, Age, SibSp, Parch and Fare). Support vectors are data points from each class that are closest to the hyperplane. They play a crucial role in determining the optimal hyperplane and maximizing the margin between classes - the distance between the hyperplane and the nearest data point from each class.\n",
    "\n",
    "There was unsuccesfull attempt to implement SVM class from scratch. A Decision was made to implement model using only scikit-learn package functions. Nevertheless, different kernel functions were used: linear, polynomial, radial basis function (RBF) and sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [571, 606]",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[219], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m### linear kernel\u001B[39;00m\n\u001B[0;32m      2\u001B[0m svm_model \u001B[38;5;241m=\u001B[39m SVC(kernel\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlinear\u001B[39m\u001B[38;5;124m'\u001B[39m)  \u001B[38;5;66;03m#try different kernels\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[43msvm_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# making predictions on the test set\u001B[39;00m\n\u001B[0;32m      6\u001B[0m predictions \u001B[38;5;241m=\u001B[39m svm_model\u001B[38;5;241m.\u001B[39mpredict(X_test)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\svm\\_base.py:192\u001B[0m, in \u001B[0;36mBaseLibSVM.fit\u001B[1;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[0;32m    190\u001B[0m     check_consistent_length(X, y)\n\u001B[0;32m    191\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 192\u001B[0m     X, y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_data\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    193\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    194\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    195\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat64\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    196\u001B[0m \u001B[43m        \u001B[49m\u001B[43morder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mC\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    197\u001B[0m \u001B[43m        \u001B[49m\u001B[43maccept_sparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcsr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    198\u001B[0m \u001B[43m        \u001B[49m\u001B[43maccept_large_sparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    199\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    201\u001B[0m y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_targets(y)\n\u001B[0;32m    203\u001B[0m sample_weight \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39masarray(\n\u001B[0;32m    204\u001B[0m     [] \u001B[38;5;28;01mif\u001B[39;00m sample_weight \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m sample_weight, dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mfloat64\n\u001B[0;32m    205\u001B[0m )\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py:584\u001B[0m, in \u001B[0;36mBaseEstimator._validate_data\u001B[1;34m(self, X, y, reset, validate_separately, **check_params)\u001B[0m\n\u001B[0;32m    582\u001B[0m         y \u001B[38;5;241m=\u001B[39m check_array(y, input_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcheck_y_params)\n\u001B[0;32m    583\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 584\u001B[0m         X, y \u001B[38;5;241m=\u001B[39m check_X_y(X, y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcheck_params)\n\u001B[0;32m    585\u001B[0m     out \u001B[38;5;241m=\u001B[39m X, y\n\u001B[0;32m    587\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m check_params\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mensure_2d\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mTrue\u001B[39;00m):\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py:1124\u001B[0m, in \u001B[0;36mcheck_X_y\u001B[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001B[0m\n\u001B[0;32m   1106\u001B[0m X \u001B[38;5;241m=\u001B[39m check_array(\n\u001B[0;32m   1107\u001B[0m     X,\n\u001B[0;32m   1108\u001B[0m     accept_sparse\u001B[38;5;241m=\u001B[39maccept_sparse,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1119\u001B[0m     input_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mX\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   1120\u001B[0m )\n\u001B[0;32m   1122\u001B[0m y \u001B[38;5;241m=\u001B[39m _check_y(y, multi_output\u001B[38;5;241m=\u001B[39mmulti_output, y_numeric\u001B[38;5;241m=\u001B[39my_numeric, estimator\u001B[38;5;241m=\u001B[39mestimator)\n\u001B[1;32m-> 1124\u001B[0m \u001B[43mcheck_consistent_length\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1126\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m X, y\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py:397\u001B[0m, in \u001B[0;36mcheck_consistent_length\u001B[1;34m(*arrays)\u001B[0m\n\u001B[0;32m    395\u001B[0m uniques \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39munique(lengths)\n\u001B[0;32m    396\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(uniques) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m--> 397\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    398\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFound input variables with inconsistent numbers of samples: \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    399\u001B[0m         \u001B[38;5;241m%\u001B[39m [\u001B[38;5;28mint\u001B[39m(l) \u001B[38;5;28;01mfor\u001B[39;00m l \u001B[38;5;129;01min\u001B[39;00m lengths]\n\u001B[0;32m    400\u001B[0m     )\n",
      "\u001B[1;31mValueError\u001B[0m: Found input variables with inconsistent numbers of samples: [571, 606]"
     ]
    }
   ],
   "source": [
    "### linear kernel\n",
    "svm_model = SVC(kernel='linear')  #try different kernels\n",
    "svm_model.fit(X_train, Y_train)\n",
    "\n",
    "# making predictions on the test set\n",
    "predictions = svm_model.predict(X_test)\n",
    "\n",
    "# evaluating accuracies\n",
    "accuracy = svm_model.score(X_test, Y_test)\n",
    "print(f\"Accuracy of linear kernel SVM: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [571, 606]",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[220], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m### poly kernel\u001B[39;00m\n\u001B[0;32m      2\u001B[0m svm_model \u001B[38;5;241m=\u001B[39m SVC(kernel\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpoly\u001B[39m\u001B[38;5;124m'\u001B[39m)  \u001B[38;5;66;03m#try different kernels\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[43msvm_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m predictions \u001B[38;5;241m=\u001B[39m svm_model\u001B[38;5;241m.\u001B[39mpredict(X_test)\n\u001B[0;32m      5\u001B[0m accuracy \u001B[38;5;241m=\u001B[39m svm_model\u001B[38;5;241m.\u001B[39mscore(X_test, Y_test)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\svm\\_base.py:192\u001B[0m, in \u001B[0;36mBaseLibSVM.fit\u001B[1;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[0;32m    190\u001B[0m     check_consistent_length(X, y)\n\u001B[0;32m    191\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 192\u001B[0m     X, y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_data\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    193\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    194\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    195\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat64\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    196\u001B[0m \u001B[43m        \u001B[49m\u001B[43morder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mC\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    197\u001B[0m \u001B[43m        \u001B[49m\u001B[43maccept_sparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcsr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    198\u001B[0m \u001B[43m        \u001B[49m\u001B[43maccept_large_sparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    199\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    201\u001B[0m y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_targets(y)\n\u001B[0;32m    203\u001B[0m sample_weight \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39masarray(\n\u001B[0;32m    204\u001B[0m     [] \u001B[38;5;28;01mif\u001B[39;00m sample_weight \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m sample_weight, dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mfloat64\n\u001B[0;32m    205\u001B[0m )\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py:584\u001B[0m, in \u001B[0;36mBaseEstimator._validate_data\u001B[1;34m(self, X, y, reset, validate_separately, **check_params)\u001B[0m\n\u001B[0;32m    582\u001B[0m         y \u001B[38;5;241m=\u001B[39m check_array(y, input_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcheck_y_params)\n\u001B[0;32m    583\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 584\u001B[0m         X, y \u001B[38;5;241m=\u001B[39m check_X_y(X, y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcheck_params)\n\u001B[0;32m    585\u001B[0m     out \u001B[38;5;241m=\u001B[39m X, y\n\u001B[0;32m    587\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m check_params\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mensure_2d\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mTrue\u001B[39;00m):\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py:1124\u001B[0m, in \u001B[0;36mcheck_X_y\u001B[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001B[0m\n\u001B[0;32m   1106\u001B[0m X \u001B[38;5;241m=\u001B[39m check_array(\n\u001B[0;32m   1107\u001B[0m     X,\n\u001B[0;32m   1108\u001B[0m     accept_sparse\u001B[38;5;241m=\u001B[39maccept_sparse,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1119\u001B[0m     input_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mX\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   1120\u001B[0m )\n\u001B[0;32m   1122\u001B[0m y \u001B[38;5;241m=\u001B[39m _check_y(y, multi_output\u001B[38;5;241m=\u001B[39mmulti_output, y_numeric\u001B[38;5;241m=\u001B[39my_numeric, estimator\u001B[38;5;241m=\u001B[39mestimator)\n\u001B[1;32m-> 1124\u001B[0m \u001B[43mcheck_consistent_length\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1126\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m X, y\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py:397\u001B[0m, in \u001B[0;36mcheck_consistent_length\u001B[1;34m(*arrays)\u001B[0m\n\u001B[0;32m    395\u001B[0m uniques \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39munique(lengths)\n\u001B[0;32m    396\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(uniques) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m--> 397\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    398\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFound input variables with inconsistent numbers of samples: \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    399\u001B[0m         \u001B[38;5;241m%\u001B[39m [\u001B[38;5;28mint\u001B[39m(l) \u001B[38;5;28;01mfor\u001B[39;00m l \u001B[38;5;129;01min\u001B[39;00m lengths]\n\u001B[0;32m    400\u001B[0m     )\n",
      "\u001B[1;31mValueError\u001B[0m: Found input variables with inconsistent numbers of samples: [571, 606]"
     ]
    }
   ],
   "source": [
    "### poly kernel\n",
    "svm_model = SVC(kernel='poly')  #try different kernels\n",
    "svm_model.fit(X_train, Y_train)\n",
    "predictions = svm_model.predict(X_test)\n",
    "accuracy = svm_model.score(X_test, Y_test)\n",
    "print(f\"Accuracy of poly kernel SVM: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [571, 606]",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[221], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m### Radial Basis Function kernel\u001B[39;00m\n\u001B[0;32m      2\u001B[0m svm_model \u001B[38;5;241m=\u001B[39m SVC(kernel\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrbf\u001B[39m\u001B[38;5;124m'\u001B[39m)  \u001B[38;5;66;03m#try different kernels\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[43msvm_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m predictions \u001B[38;5;241m=\u001B[39m svm_model\u001B[38;5;241m.\u001B[39mpredict(X_test)\n\u001B[0;32m      5\u001B[0m accuracy \u001B[38;5;241m=\u001B[39m svm_model\u001B[38;5;241m.\u001B[39mscore(X_test, Y_test)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\svm\\_base.py:192\u001B[0m, in \u001B[0;36mBaseLibSVM.fit\u001B[1;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[0;32m    190\u001B[0m     check_consistent_length(X, y)\n\u001B[0;32m    191\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 192\u001B[0m     X, y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_data\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    193\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    194\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    195\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat64\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    196\u001B[0m \u001B[43m        \u001B[49m\u001B[43morder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mC\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    197\u001B[0m \u001B[43m        \u001B[49m\u001B[43maccept_sparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcsr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    198\u001B[0m \u001B[43m        \u001B[49m\u001B[43maccept_large_sparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    199\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    201\u001B[0m y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_targets(y)\n\u001B[0;32m    203\u001B[0m sample_weight \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39masarray(\n\u001B[0;32m    204\u001B[0m     [] \u001B[38;5;28;01mif\u001B[39;00m sample_weight \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m sample_weight, dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mfloat64\n\u001B[0;32m    205\u001B[0m )\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py:584\u001B[0m, in \u001B[0;36mBaseEstimator._validate_data\u001B[1;34m(self, X, y, reset, validate_separately, **check_params)\u001B[0m\n\u001B[0;32m    582\u001B[0m         y \u001B[38;5;241m=\u001B[39m check_array(y, input_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcheck_y_params)\n\u001B[0;32m    583\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 584\u001B[0m         X, y \u001B[38;5;241m=\u001B[39m check_X_y(X, y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcheck_params)\n\u001B[0;32m    585\u001B[0m     out \u001B[38;5;241m=\u001B[39m X, y\n\u001B[0;32m    587\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m check_params\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mensure_2d\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mTrue\u001B[39;00m):\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py:1124\u001B[0m, in \u001B[0;36mcheck_X_y\u001B[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001B[0m\n\u001B[0;32m   1106\u001B[0m X \u001B[38;5;241m=\u001B[39m check_array(\n\u001B[0;32m   1107\u001B[0m     X,\n\u001B[0;32m   1108\u001B[0m     accept_sparse\u001B[38;5;241m=\u001B[39maccept_sparse,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1119\u001B[0m     input_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mX\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   1120\u001B[0m )\n\u001B[0;32m   1122\u001B[0m y \u001B[38;5;241m=\u001B[39m _check_y(y, multi_output\u001B[38;5;241m=\u001B[39mmulti_output, y_numeric\u001B[38;5;241m=\u001B[39my_numeric, estimator\u001B[38;5;241m=\u001B[39mestimator)\n\u001B[1;32m-> 1124\u001B[0m \u001B[43mcheck_consistent_length\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1126\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m X, y\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py:397\u001B[0m, in \u001B[0;36mcheck_consistent_length\u001B[1;34m(*arrays)\u001B[0m\n\u001B[0;32m    395\u001B[0m uniques \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39munique(lengths)\n\u001B[0;32m    396\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(uniques) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m--> 397\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    398\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFound input variables with inconsistent numbers of samples: \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    399\u001B[0m         \u001B[38;5;241m%\u001B[39m [\u001B[38;5;28mint\u001B[39m(l) \u001B[38;5;28;01mfor\u001B[39;00m l \u001B[38;5;129;01min\u001B[39;00m lengths]\n\u001B[0;32m    400\u001B[0m     )\n",
      "\u001B[1;31mValueError\u001B[0m: Found input variables with inconsistent numbers of samples: [571, 606]"
     ]
    }
   ],
   "source": [
    "### Radial Basis Function kernel\n",
    "svm_model = SVC(kernel='rbf')  #try different kernels\n",
    "svm_model.fit(X_train, Y_train)\n",
    "predictions = svm_model.predict(X_test)\n",
    "accuracy = svm_model.score(X_test, Y_test)\n",
    "print(f\"Accuracy of Radial Basis Function kernel SVM: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of sigmoid kernel SVM: 0.6203703703703703\n"
     ]
    }
   ],
   "source": [
    "### sigmoid kernel\n",
    "svm_model = SVC(kernel='sigmoid')  #try different kernels\n",
    "svm_model.fit(X_train, Y_train)\n",
    "predictions = svm_model.predict(X_test)\n",
    "accuracy = svm_model.score(X_test, Y_test)\n",
    "print(f\"Accuracy of sigmoid kernel SVM: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best accuracy was reached for linear kernel SVM, and it was used for final prediction for Kaggle competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     PassengerId  Pclass  Sex       Age  SibSp  Parch      Fare\n",
      "0            892       3    0  34.50000      0      0    7.8292\n",
      "1            893       3    1  47.00000      1      0    7.0000\n",
      "2            894       2    0  62.00000      0      0    9.6875\n",
      "3            895       3    0  27.00000      0      0    8.6625\n",
      "4            896       3    1  22.00000      1      1   12.2875\n",
      "..           ...     ...  ...       ...    ...    ...       ...\n",
      "413         1305       3    0  30.27259      0      0    8.0500\n",
      "414         1306       1    1  39.00000      0      0  108.9000\n",
      "415         1307       3    0  38.50000      0      0    7.2500\n",
      "416         1308       3    0  30.27259      0      0    8.0500\n",
      "417         1309       3    0  30.27259      1      1   22.3583\n",
      "\n",
      "[418 rows x 7 columns]\n",
      "\n",
      "NaN rows in test set:\n",
      "Empty DataFrame\n",
      "Columns: [PassengerId, Pclass, Name, Sex, Age, SibSp, Parch, Ticket, Fare, Cabin, Embarked]\n",
      "Index: []\n",
      "Your submission was successfully saved!\n"
     ]
    }
   ],
   "source": [
    "# save the results\n",
    "predictions = svm_model.predict(df_test_processed[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare']].values)\n",
    "output = pd.DataFrame({'PassengerId': df_test_processed.PassengerId, 'Survived': predictions})\n",
    "output.to_csv('SVM_kaggle_submission.csv', index=False)\n",
    "print(\"Your submission was successfully saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "efv0crrNC7Aw"
   },
   "source": [
    "#### Random Forest Model\n",
    "##### To do:\n",
    "- arono"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random forest (kaggle example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Random Forest Classifier: 0.8159371492704826\n",
      "Your submission was successfully saved!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "test_data = df_test\n",
    "y = df_train[\"Survived\"]\n",
    "\n",
    "features = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\n",
    "X = pd.get_dummies(df_train[features])\n",
    "X_test = pd.get_dummies(test_data[features])\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\n",
    "model.fit(X, y)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "accuracy = model.score(X, y)\n",
    "print(f\"Accuracy of Random Forest Classifier: {accuracy}\")\n",
    "\n",
    "output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\n",
    "output.to_csv('RF_kaggle_submission.csv', index=False)\n",
    "print(\"Your submission was successfully saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UaJFk54ICW0c"
   },
   "source": [
    "#### Analysis of results\n",
    "\n",
    "JAK PCA TO MOŻNA BIBLIOTEK UŻYĆ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "To do:\n",
    "- did we reachedour goals?\n",
    "- what is interesting in our solutions\n",
    "- what could be done better? limits and possible improvement"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
